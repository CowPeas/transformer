# transformer
Self-attention is a mechanism that computes the attention weights between each input element and all the other input elements, and then combines the input elements according to these attention weights.

The key innovation in the transformer architecture is the introduction of self-attention, which allows the model to selectively attend to different parts of the input sequence when generating each output element. 

The transformer architecture consists of an encoder and a decoder. The encoder takes the input sequence and generates a sequence of hidden states, while the decoder takes the encoder output and generates the output sequence. 
