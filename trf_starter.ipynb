{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrating the transformer architecture on a small subset of the WMT English-German parallel corpus using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Masking, Dropout, LayerNormalization\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = ['hello world', 'goodbye world', 'how are you']\n",
    "target_seq = ['hallo welt', 'auf wiedersehen welt', 'wie geht es dir']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='', char_level=True)\n",
    "tokenizer.fit_on_texts(input_seq + target_seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text sequences to integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = tokenizer.texts_to_sequences(input_seq)\n",
    "target_seq = tokenizer.texts_to_sequences(target_seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len = max(len(seq) for seq in input_seq + target_seq)\n",
    "input_seq = pad_sequences(input_seq, maxlen=max_len, padding='post')\n",
    "target_seq = pad_sequences(target_seq, maxlen=max_len, padding='post')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_input = Input(shape=(max_len,))\n",
    "decoder_input = Input(shape=(max_len,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim = 32\n",
    "embedding_encoder = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, mask_zero=True)(encoder_input)\n",
    "embedding_decoder = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, mask_zero=True)(decoder_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define masking layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_encoder = Masking(mask_value=0.0)(embedding_encoder)\n",
    "masking_decoder = Masking(mask_value=0.0)(embedding_decoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define multi-head attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "key_dim = 32\n",
    "value_dim = 32\n",
    "attention_encoder = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, value_dim=value_dim)(masking_encoder, masking_encoder)\n",
    "attention_decoder = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, value_dim=value_dim)(masking_decoder, masking_decoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define feedforward layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feedforward_dim = 64\n",
    "feedforward_encoder = TimeDistributed(Dense(feedforward_dim, activation='relu'))(attention_encoder)\n",
    "feedforward_encoder = TimeDistributed(Dropout(0.5))(feedforward_encoder)\n",
    "feedforward_encoder = TimeDistributed(Dense(embedding_dim))(feedforward_encoder)\n",
    "\n",
    "feedforward_decoder = TimeDistributed(Dense(feedforward_dim, activation='relu'))(attention_decoder)\n",
    "feedforward_decoder = TimeDistributed(Dropout(0.5))(feedforward_decoder)\n",
    "feedforward_decoder = TimeDistributed(Dense(embedding_dim))(feedforward_decoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define concatenation layer\n",
    "concatenation = Concatenate(axis=1)([feedforward_encoder, feedforward_decoder])\n",
    "\n",
    "# Define normalization layer\n",
    "normalization = LayerNormalization()(concatenation)\n",
    "\n",
    "# Define output layer\n",
    "output = TimeDistributed(Dense(len(tokenizer.word_index) + 1, activation='softmax'))(normalization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model(inputs=[encoder_input, decoder_input], outputs=output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "history = model.fit([input_seq[:num_samples], target_seq[:num_samples]], target_seq[:num_samples], epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loss, test_acc = model.evaluate([input_seq[num_samples:], target_seq[num_samples:]], target_seq[num_samples:], verbose=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
